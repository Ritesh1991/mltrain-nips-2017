{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Extract VQA features from prerained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Image feature using pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You are welcome to use any pretrained <a href=\"https://github.com/apache/incubator-mxnet/tree/master/example/image-classification\">image classification</a>/<a href=\"https://github.com/apache/incubator-mxnet/tree/master/example/rcnn\">object detection</a> model to extract MSCOCO image features. \n",
    "\n",
    "We use pretrained resnet-152 network to get the image features. Please refer to <a href=\"https://github.com/akirafukui/vqa-mcb/tree/master/preprocess\">VQA-MCB</a>. After preprocessing, you should have processed image features stored in .jpg.npz files. Image feature size is $2048 \\times 14 \\times 14$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question feature using pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You need to download original text datasets(annotation and question) from <a href=\"http://www.visualqa.org/vqa_v1_download.html\">VQA</a> first and run \n",
    "```python\n",
    "python textpreprocess.py\n",
    "```\n",
    "to get __vqa_raw_train.json__, __vqa_raw_test.json__ and __vqa_raw_val.json__.\n",
    "\n",
    "Once you have these, run\n",
    "```python\n",
    "python prepro.py --input_train_json vqa_raw_train.json --input_val_json vqa_raw_val.json --input_test_json vqa_raw_test.json --num_ans 1000\n",
    "```\n",
    "to get the question features. __--num_ans__ specifiy how many top answers you want to use during training. This will generate two files in your main folder, data_prepro.h5 and data_prepro.json. data_prepro.h5 contains questions and answers for train, validation and test sets. data_prepro.json contains index map for all words in questions and answers.\n",
    "\n",
    "You can then feed the __indexed question feature__ to pretrained language model such as <a href=\"https://github.com/apache/incubator-mxnet/tree/master/example/rnn\">LSTM</a> or skipthought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use <a href=\"https://github.com/awslabs/sockeye\">neural machine translation system</a> to extract the question features. The pretrained NMT model is trained on UN parallel corpus 1.0, English/Spanish to French. Instead of using indexed question feature as input, we use <a href=\"https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perltokenized\">tokenized</a> question string as input. \n",
    "\n",
    "(1)Tokenize the question dataset ans save as a txt file.\n",
    "\n",
    "(2)Install the <a href=\"https://github.com/awslabs/sockeye\">sockeye model</a>.\n",
    "\n",
    "(3)Download the pretrained model\n",
    "```python\n",
    "url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/VQA-notebook/model.zip'\n",
    "download(url_format,overwrite=True)\n",
    "unzip -q model.zip\n",
    "```\n",
    "\n",
    "(4)Change the path of the tokenized txt file in extract_encoder.py and run\n",
    "```python\n",
    "python extract_encoder.py\n",
    "```\n",
    "We will then get extracted encoder in a .txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "![](img/nmt.png )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feel Lucky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are many versions of preprocessed extracted features.\n",
    "One is from VT-vision-lab's <a href=\"https://github.com/VT-vision-lab/VQA_LSTM_CNN\">VQA_LSTM_CNN</a>:\n",
    "\n",
    "Under Evaluation section, you can download the features. You will see three files in the folder: data_prepro.h5, data_prepro.json and data_img.h5. data_prepro.h5 contains questions and answers for train and test sets. data_prepro.json contains index map for all words in questions and answers. data_img.h5 contains image features using pretrained VGG19 network. Image feature size is 4096. Question feature is a vector of intergers that maps to words in the question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
