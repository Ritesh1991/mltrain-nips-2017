{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages\n",
      "Requirement already satisfied: requests in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from mxnet)\n",
      "Requirement already satisfied: numpy in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from mxnet)\n",
      "Requirement already satisfied: graphviz in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from mxnet)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from requests->mxnet)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from requests->mxnet)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from requests->mxnet)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nbcommon/anaconda2_501/lib/python2.7/site-packages (from requests->mxnet)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This notebook has two cells. First run the cell below this cell which has all the function definitions. ###\n",
    "### Then run this cell to get the desired output. ###\n",
    "\n",
    "# author: Ashish Khetan, khetan2@illinois.edu, Zachary C. Lipton, Animashree Anandkumar\n",
    "\n",
    "# This notebook is for the algorithm (MBEM) proposed in the paper \"Learning From Noisy Singly-labeled Data\"\n",
    "# that is under review at ICLR 2018. The paper can be obtained at \"https://openreview.net/pdf?id=S15IBlWAZ\".\n",
    "\n",
    "# Model Bootstrapped Expectation Maximization (MBEM) is a new algorithm for \n",
    "# training a deep learning model using noisy data collected from crowdsourcing \n",
    "# platforms such as Amazon Mechanical Turk. MBEM outperforms classical crowdsourcing algorithm \"majority vote\".\n",
    "\n",
    "# In this notebook, we run MBEM on CIFAR-10 dataset. We synthetically generate noisy labels given the true labels \n",
    "# and using hammer-spammer worker distribution for worker qualities that is explained in the paper.\n",
    "# Under the setting when the total annotation budget is fixed, that is we choose whether to collect \"1\" noisy label \n",
    "# for each of the \"n\" training samples or collect \"r\" noisy labels for each of the \"n/r\" training examples,\n",
    "# we show empirically that it is better to choose the former case, that is collect \"1\" noisy label per example \n",
    "# for as many training examples as possible when the total annotation budget is fixed.\n",
    "\n",
    "# It takes a few hours to run this notebook and obtain the desired numerical results when using gpus.\n",
    "# We use ResNet deep learning model for training a classifier for CIFAR-10.\n",
    "# We use ResNet MXNET implementation given in https://github.com/tornadomeet/ResNet/. \n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging,os\n",
    "import copy\n",
    "import urllib\n",
    "import logging,os,sys\n",
    "from scipy import stats\n",
    "from random import shuffle\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "def generate_workers(m,k,gamma,class_wise):\n",
    "    # Generating worker confusion matrices according to class-wise hammer-spammer distribution if class_wise ==1\n",
    "    # Generating worker confusion matrices according to hammer-spammer distribution if class_wise ==0    \n",
    "    # One row for each true class and columns for given answers\n",
    "    \n",
    "    #iniializing confusion matrices with all entries being equal to 1/k that is corresponding to a spammer worker.\n",
    "    conf = (1/float(k))*np.ones((m,k,k))\n",
    "    # a loop to generate confusion matrix for each worker \n",
    "    for i in range(m): \n",
    "        # if class_wise ==0 then generating worker confusion matrix according to hammer-spammer distribution\n",
    "        if(class_wise==0):\n",
    "            #letting the confusion matrix to be identity with probability gamma \n",
    "            if(np.random.uniform(0,1) < gamma):\n",
    "                conf[i] = np.identity(k)\n",
    "            # To avoid numerical issues changing the spammer matrix each element slightly    \n",
    "            else:\n",
    "                conf[i] = conf[i] + 0.01*np.identity(k)\n",
    "                conf[i] = np.divide(conf[i],np.outer(np.sum(conf[i],axis =1),np.ones(k)))        \n",
    "        else:\n",
    "            # if class_wise ==1 then generating each class separately according to hammer-spammer distribution    \n",
    "            for j in range(k):\n",
    "                # with probability gamma letting the worker to be hammer for the j-th class\n",
    "                if(np.random.uniform(0,1) < gamma):\n",
    "                    conf[i,j,:] = 0\n",
    "                    conf[i,j,j] = 1 \n",
    "                # otherwise letting the worker to be spammer for the j-th class. \n",
    "                # again to avoid numerical issues changing the spammer distribution slighltly \n",
    "                # by generating uniform random variable between 0.1 and 0.11\n",
    "                else:\n",
    "                    conf[i,j,:] = 1\n",
    "                    conf[i,j,j] = 1 + np.random.uniform(0.1,0.11)\n",
    "                    conf[i,j,:] = conf[i,j,:]/np.sum(conf[i,j,:])\n",
    "    # returining the confusion matrices \n",
    "    return conf\n",
    "\n",
    "# Downloading data for CIFAR10\n",
    "# The following function downloads .rec iterator and .lst files (MXNET iterators) for CIFAR10 \n",
    "# that are used for training the deep learning model with noisy annotations\n",
    "def download_cifar10():\n",
    "    fname = ['train.rec', 'train.lst', 'val.rec', 'val.lst']\n",
    "    testfile = urllib.URLopener()\n",
    "    testfile.retrieve('http://data.mxnet.io/data/cifar10/cifar10_train.rec', fname[0])\n",
    "    testfile.retrieve('http://data.mxnet.io/data/cifar10/cifar10_train.lst', fname[1])\n",
    "    testfile.retrieve('http://data.mxnet.io/data/cifar10/cifar10_val.rec',   fname[2])\n",
    "    testfile.retrieve('http://data.mxnet.io/data/cifar10/cifar10_val.lst',   fname[3])\n",
    "    return fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### main function ####    \n",
    "def main(fname,n,n1,k,conf,samples,repeat,epochs,depth,gpus):    \n",
    "    # defining the range of samples that are to be used for training the model\n",
    "    valid = np.arange(0,samples)\n",
    "    # declaring the other samples to be invalid \n",
    "    invalid = np.arange(samples,n)\n",
    "\n",
    "    # calling function generate_labels_weight which generates noisy labels given the true labels \n",
    "    # the true lables of the examples are ascertained from the .lst files \n",
    "    # it takes as input the following:\n",
    "    # name of the .lst files for the training set and the validation set\n",
    "    # conf: the confusion matrices of the workers\n",
    "    # repeat: number of redundant labels that need to be generated for each sample\n",
    "    # for each i-th sample repeat number of workers are chosen randomly that labels the given sample\n",
    "    # it returns a multi dimensional array resp_org: \n",
    "    # such that resp_org[i,j,k] is 0 vector if the a-th worker was not chosen to label the i-th example\n",
    "    # else it is one-hot representation of the noisy label given by the j-th worker on the i-th example\n",
    "    # workers_train_label_org: it is a dictionary. it contains \"repeat\" number of numpy arrays, each of size (n,k)\n",
    "    # the arrays have the noisy labels given by the workers\n",
    "    # workers_val_label: it is a dictionary. it contains one numpy array of size (n,k) \n",
    "    # that has true label of the examples in the validation set\n",
    "    # workers_this_example: it is a numpy array of size (n,repeat).\n",
    "    # it conatins identity of the worker that are used to generate \"repeat\" number of noisy labels for example    \n",
    "    resp_org, workers_train_label_org, workers_val_label, workers_this_example = generate_labels_weight(fname,n,n1,repeat,conf)    \n",
    "    #setting invalid ones 0, so that they are not used by deep learning module\n",
    "    for r in range(repeat):\n",
    "        workers_train_label_org['softmax'+ str(r) +'_label'][invalid] = 0       \n",
    "    \n",
    "    print \"Algorithm: majority vote:\\t\\t\",\n",
    "    # running the baseline algorithm where the noisy labels are aggregated using the majority voting\n",
    "    # calling majority voting function to aggregate the noisy labels\n",
    "    pred_mv = majority_voting(resp_org[valid])    \n",
    "    # call_train function takes as input the noisy labels \"pred_mv\", trains ResNet model for the given \"depth\"\n",
    "    # for \"epochs\" run using the available \"gpus\". \n",
    "    # it prints the generalization error of the trained model.\n",
    "    _, val_acc = call_train(n,samples,k,pred_mv,workers_val_label,fname,epochs,depth,gpus)\n",
    "    print \"generalization_acc:  \" + str(val_acc)\n",
    "    \n",
    "    print \"Algorithm: weighted majority vote:\\t\", \n",
    "    # running the another baseline algorithm where the aggregation is performed using the weighted majority vote\n",
    "    # creating a numpy array to store weighted majority vote labels\n",
    "    naive_agg = np.zeros((n,k))\n",
    "    # generating the weighted majority vote label using the original noisy labels stored in the \n",
    "    # dictionary \"workers_train_label_org\"\n",
    "    for r in range(repeat):\n",
    "        naive_agg = naive_agg + (1/repeat)*copy.deepcopy(workers_train_label_org['softmax'+ str(r) +'_label']) \n",
    "    # calling the \"call_train\" function which besides printing the generalization error \n",
    "    # returns model prediction on the training examples, which is being stored in the variable \"naive_pred\".\n",
    "    naive_pred, val_acc = call_train(n,samples,k,naive_agg[valid],workers_val_label,fname,epochs,depth,gpus)\n",
    "    print \"generalization_acc:  \" + str(val_acc)\n",
    "\n",
    "    print \"Algorithm: MBEM:\\t\\t\\t\",    \n",
    "    # running the proposed algorithm \"MBEM: model bootstrapped expectation maximization\" \n",
    "    # computing posterior probabilities of the true labels given the noisy labels and the worker identities.\n",
    "    # post_prob_DS function takes the noisy labels given by the workers \"resp_org\", model prediction obtained \n",
    "    # by running \"weighted majority vote\" algorithm, and the worker identities.\n",
    "    probs_est_labels = post_prob_DS(resp_org[valid],naive_pred[valid],workers_this_example[valid])      \n",
    "    algo_agg = np.zeros((n,k))    \n",
    "    algo_agg[valid] = probs_est_labels\n",
    "    # calling the \"call_train\" function with aggregated labels being the posterior probability distribution of the \n",
    "    # examples given the model prediction obtained using the \"weighted majority vote\" algorithm.\n",
    "    _, val_acc = call_train(n,samples,k,algo_agg[valid],workers_val_label,fname,epochs,depth,gpus)\n",
    "    print \"generalization_acc:  \" + str(val_acc)\n",
    "    \n",
    "def call_train(n,samples,k,workers_train_label_use,workers_val_label,fname,epochs,depth,gpus):\n",
    "    # this function takes as input aggregated labels of the training examples\n",
    "    # along with name of the .rec files for training the ResNet model, depth of the model, number of epochs, and gpus information\n",
    "    # it returns model prediction on the training examples.\n",
    "    # we train the model twice first using the given aggregated labels and\n",
    "    # second using the model prediction on the training examples on based on the first training\n",
    "    # this aspect is not covered in the algorithm given in the paper. however, it works better in practice.\n",
    "    # training the model twice in this fashion can be replaced by training once for sufficiently large number of epochs\n",
    "    \n",
    "    # first training of the model using the given aggregated labels \n",
    "    workers_train_label_use_core = np.zeros((n,k))\n",
    "    workers_train_label_use_core[np.arange(samples)] = workers_train_label_use        \n",
    "    pred_first_iter, val_acc = call_train_core(n,samples,k,workers_train_label_use_core,workers_val_label,fname,epochs,depth,gpus)\n",
    "    # second training of the model using the model prediction on the training examples based on the first training.\n",
    "    workers_train_label_use_core = np.zeros((n,k))\n",
    "    workers_train_label_use_core[np.arange(samples)] = pred_first_iter[np.arange(samples)]\n",
    "    pred_second_iter, val_acc = call_train_core(n,samples,k,workers_train_label_use_core,workers_val_label,fname,epochs,depth,gpus)\n",
    "    return pred_second_iter, val_acc\n",
    "    \n",
    "def call_train_core(n,samples,k,workers_train_label_use_core,workers_val_label,fname,epochs,depth,gpus):\n",
    "    # this function takes as input the same variables as the \"call_train\" function and it calls\n",
    "    # the mxnet implementation of ResNet training module function \"train\" \n",
    "    workers_train_label = {} \n",
    "    workers_train_label['softmax0_label'] = workers_train_label_use_core  \n",
    "    prediction, val_acc = train(gpus,fname,workers_train_label,workers_val_label,numepoch=epochs,batch_size=500,depth = depth,lr=0.5)\n",
    "    model_pred = np.zeros((n,k))\n",
    "    model_pred[np.arange(samples), np.argmax(prediction[0:samples],1)] = 1\n",
    "    return model_pred, val_acc \n",
    "\n",
    "\n",
    "def generate_labels_weight(fname,n,n1,repeat,conf):\n",
    "    # extracting the number of workers and the number of classes from the confusion matrices\n",
    "    m, k  = conf.shape[0], conf.shape[1]    \n",
    "    # a numpy array to store true class of the training examples\n",
    "    class_train = np.zeros((n), dtype = np.int)\n",
    "    # reading the train.lst file and storing true class of each training example\n",
    "    with open(fname[1],\"r\") as f1:\n",
    "        content = f1.readlines()\n",
    "    for i in range(n):\n",
    "        content_lst = content[i].split(\"\\t\")\n",
    "        class_train[i] = int(float(content_lst[1]))\n",
    "    \n",
    "    # a dictionary to store noisy labels generated using the worker confusion matrices for each training example  \n",
    "    workers_train_label = {}\n",
    "    # the dictionary contains \"repeat\" number of numpy arrays with keys named \"softmax_0_label\", where 0 varies\n",
    "    # each array has the noisy labels for the training examples given by the workers\n",
    "    for i in range(repeat):\n",
    "        workers_train_label['softmax' + str(i) + '_label'] = np.zeros((n,k))   \n",
    "    \n",
    "    # Generating noisy labels according the worker confusion matrices and the true labels of the examples\n",
    "    # a variable to store one-hot noisy label, note that each label belongs to one of the k classes\n",
    "    resp = np.zeros((n,m,k))\n",
    "    # a variable to store identity of the workers that are assigned to the i-th example\n",
    "    # note that \"repeat\" number of workers are randomly chosen from the set of [m] workers and assigned to each example\n",
    "    workers_this_example = np.zeros((n,repeat),dtype=np.int)\n",
    "    \n",
    "    # iterating over each training example\n",
    "    for i in range(n):\n",
    "        # randomly selecting \"repeat\" number of workers for the i-th example\n",
    "        workers_this_example[i] = np.sort(np.random.choice(m,repeat,replace=False))\n",
    "        count = 0\n",
    "        # for each randomly chosen worker generating noisy label according to her confusion matrix and the true label\n",
    "        for j in workers_this_example[i]:\n",
    "            # using the row of the confusion matrix corresponding to the true label generating the noisy label\n",
    "            temp_rand = np.random.multinomial(1,conf[j,class_train[i],:])\n",
    "            # storing the noisy label in the resp variable \n",
    "            resp[i,j,:] = temp_rand\n",
    "            # storing the noisy label in the dictionary\n",
    "            workers_train_label['softmax' + str(count) + '_label'][i] = temp_rand\n",
    "            count = count +1 \n",
    "            \n",
    "    # note that in the dictionary each numpy array is of size only (n,k). \n",
    "    # The dictionary is passed to the deep learning module\n",
    "    # however, the resp variable is a numpy array of size (n,m,k).\n",
    "    # it is used for performing expectation maximization on the noisy labels\n",
    "\n",
    "    # initializing a dictionary to store one-hot representation of the true labels for the validation set\n",
    "    workers_val_label = {}\n",
    "    # the dictionary contains \"repeat\" number of numpy arrays with keys named \"softmax_0_label\", where 0 varies\n",
    "    # each array has the true labels of the examples in the validation set\n",
    "    workers_val_label['softmax' + str(0) + '_label'] = np.zeros((n1,k))  \n",
    "    \n",
    "    # reading the .lst file for the validation set\n",
    "    content_val_lst = np.genfromtxt(fname[3], delimiter='\\t')\n",
    "    # storing the true labels of the examples in the validation set in the dictionary\n",
    "    for i in range(n1):\n",
    "        workers_val_label['softmax' + str(0) + '_label'][i][int(content_val_lst[i,1])] = 1\n",
    "    \n",
    "    # returning the noisy responses of the workers stored in the resp numpy array, \n",
    "    # the noisy labels stored in the dictionary that is used by the deep learning module\n",
    "    # the true lables of the examples in the validation set stored in the dictionary\n",
    "    # identity of the workers that are assigned to th each example in the training set\n",
    "    return resp, workers_train_label, workers_val_label, workers_this_example\n",
    "\n",
    "def majority_voting(resp):\n",
    "    # computes majority voting label\n",
    "    # ties are broken uniformly at random\n",
    "    n = resp.shape[0]\n",
    "    k = resp.shape[2]\n",
    "    pred_mv = np.zeros((n), dtype = np.int)\n",
    "    for i in range(n):\n",
    "        # finding all labels that have got maximum number of votes\n",
    "        poss_pred = np.where(np.sum(resp[i],0) == np.max(np.sum(resp[i],0)))[0]\n",
    "        shuffle(poss_pred)\n",
    "        # choosing a label randomly among all the labels that have got the highest number of votes\n",
    "        pred_mv[i] = poss_pred[0]   \n",
    "    pred_mv_vec = np.zeros((n,k))\n",
    "    # returning one-hot representation of the majority vote label\n",
    "    pred_mv_vec[np.arange(n), pred_mv] = 1\n",
    "    return pred_mv_vec\n",
    "\n",
    "def post_prob_DS(resp_org,e_class,workers_this_example):\n",
    "    # computes posterior probability distribution of the true label given the noisy labels annotated by the workers\n",
    "    # and model prediction\n",
    "    n = resp_org.shape[0]\n",
    "    m = resp_org.shape[1]\n",
    "    k = resp_org.shape[2]\n",
    "    repeat = workers_this_example.shape[1]\n",
    "    \n",
    "    temp_class = np.zeros((n,k))\n",
    "    e_conf = np.zeros((m,k,k))\n",
    "    temp_conf = np.zeros((m,k,k))\n",
    "    \n",
    "    #Estimating confusion matrices of each worker by assuming model prediction \"e_class\" is the ground truth label\n",
    "    for i in range(n):\n",
    "        for j in workers_this_example[i]: #range(m)\n",
    "            temp_conf[j,:,:] = temp_conf[j,:,:] + np.outer(e_class[i],resp_org[i,j])\n",
    "    #regularizing confusion matrices to avoid numerical issues\n",
    "    for j in range(m):  \n",
    "        for r in range(k):\n",
    "            if (np.sum(temp_conf[j,r,:]) ==0):\n",
    "                # assuming worker is spammer for the particular class if there is no estimation for that class for that worker\n",
    "                temp_conf[j,r,:] = 1/k\n",
    "            else:\n",
    "                # assuming there is a non-zero probability of each worker assigning labels for all the classes\n",
    "                temp_conf[j,r,:][temp_conf[j,r,:]==0] = 1e-10\n",
    "        e_conf[j,:,:] = np.divide(temp_conf[j,:,:],np.outer(np.sum(temp_conf[j,:,:],axis =1),np.ones(k)))\n",
    "    # Estimating posterior distribution of the true labels using confusion matrices of the workers and the original\n",
    "    # noisy labels annotated by the workers\n",
    "    for i in range(n):\n",
    "        for j in workers_this_example[i]: \n",
    "            if (np.sum(resp_org[i,j]) ==1):\n",
    "                temp_class[i] = temp_class[i] + np.log(np.dot(e_conf[j,:,:],np.transpose(resp_org[i,j])))\n",
    "        temp_class[i] = np.exp(temp_class[i])\n",
    "        temp_class[i] = np.divide(temp_class[i],np.outer(np.sum(temp_class[i]),np.ones(k)))\n",
    "        e_class[i] = temp_class[i]           \n",
    "    return e_class\n",
    "\n",
    "# The following code implements ResNet using MXNET. It is copied from https://github.com/tornadomeet/ResNet/.\n",
    "def train(gpus,fname,workers_train_label,workers_val_label,numepoch,batch_size,depth = 20,lr=0.5):    \n",
    "    output_filename = \"tr_err.txt\"\n",
    "    model_num = 1\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if os.path.isfile(output_filename):\n",
    "        os.remove(output_filename)\n",
    "    hdlr = logging.FileHandler(output_filename)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "\n",
    "    kv = mx.kvstore.create('device')\n",
    "    ### training iterator\n",
    "    train1 = mx.io.ImageRecordIter(\n",
    "        path_imgrec         = fname[0],\n",
    "        label_width         = 1,\n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax0_label',\n",
    "        data_shape          = (3, 32, 32), \n",
    "        batch_size          = batch_size,\n",
    "        pad                 = 4, \n",
    "        fill_value          = 127,  \n",
    "        rand_crop           = True,\n",
    "        max_random_scale    = 1.0,  \n",
    "        min_random_scale    = 1.0, \n",
    "        rand_mirror         = True,\n",
    "        shuffle             = False,\n",
    "        num_parts           = kv.num_workers,\n",
    "        part_index          = kv.rank)    \n",
    "           \n",
    "    ### Validation iterator\n",
    "    val1 = mx.io.ImageRecordIter(\n",
    "        path_imgrec         = fname[2],\n",
    "        label_width         = 1,\n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax0_label', \n",
    "        batch_size          = batch_size,\n",
    "        data_shape          = (3, 32, 32), \n",
    "        rand_crop           = False,\n",
    "        rand_mirror         = False,\n",
    "        pad = 0,\n",
    "        num_parts           = kv.num_workers,\n",
    "        part_index          = kv.rank)\n",
    "\n",
    "    n = workers_train_label['softmax0_label'].shape[0]\n",
    "    k = workers_train_label['softmax0_label'].shape[1]\n",
    "    n1 = workers_val_label['softmax0_label'].shape[0]      \n",
    "    train2 = mx.io.NDArrayIter(np.zeros(n), workers_train_label, batch_size, shuffle = False,)\n",
    "    train_iter = MultiIter([train1,train2])          \n",
    "    val2 = mx.io.NDArrayIter(np.zeros(n1), workers_val_label, batch_size = batch_size,shuffle = False,)\n",
    "    val_iter = MultiIter([val1,val2]) \n",
    "        \n",
    "    if((depth-2)%6 == 0 and depth < 164):\n",
    "        per_unit = [int((depth-2)/6)]\n",
    "        filter_list = [16, 16, 32, 64]\n",
    "        bottle_neck = False\n",
    "    else:\n",
    "        raise ValueError(\"no experiments done on detph {}, you can do it youself\".format(depth))\n",
    "    units = per_unit*3\n",
    "    symbol = resnet(units=units, num_stage=3, filter_list=filter_list, num_class=k,data_type=\"cifar10\", \n",
    "                    bottle_neck = False, bn_mom=0.9, workspace=512,\n",
    "                    memonger=False)\n",
    "    \n",
    "    devs = mx.cpu() if gpus is None else [mx.gpu(int(i)) for i in gpus.split(',')]\n",
    "    epoch_size = max(int(n / batch_size / kv.num_workers), 1)\n",
    "    if not os.path.exists(\"./model\" + str(model_num)):\n",
    "        os.mkdir(\"./model\" + str(model_num))\n",
    "    model_prefix = \"model\"+ str(model_num) + \"/resnet-{}-{}-{}\".format(\"cifar10\", depth, kv.rank)\n",
    "    checkpoint = mx.callback.do_checkpoint(model_prefix)\n",
    "\n",
    "    def custom_metric(label,softmax):\n",
    "        return len(np.where(np.argmax(softmax,1)==np.argmax(label,1))[0])/float(label.shape[0])\n",
    "    #there is only one softmax layer with respect to which error of all the labels are computed\n",
    "    output_names = []\n",
    "    output_names = output_names + ['softmax' + str(0) + '_output']   \n",
    "    eval_metrics = mx.metric.CustomMetric(custom_metric,name = 'accuracy', output_names=output_names, label_names=workers_train_label.keys())    \n",
    "       \n",
    "    model = mx.mod.Module(\n",
    "        context             = devs,\n",
    "        symbol              = mx.sym.Group(symbol),\n",
    "        data_names          = ['data'],\n",
    "        label_names         = workers_train_label.keys(),#['softmax0_label']\n",
    "        )\n",
    "    lr_scheduler = multi_factor_scheduler(0, epoch_size, step=[40, 50], factor=0.1)\n",
    "    optimizer_params = {\n",
    "        'learning_rate': lr,\n",
    "        'momentum' : 0.9,\n",
    "        'wd' : 0.0001,\n",
    "        'lr_scheduler': lr_scheduler}\n",
    "       \n",
    "    model.fit(\n",
    "        train_iter,\n",
    "        eval_data          = val_iter,\n",
    "        eval_metric        = eval_metrics,\n",
    "        kvstore            = kv,\n",
    "        batch_end_callback = mx.callback.Speedometer(batch_size, 50),\n",
    "        epoch_end_callback = checkpoint,\n",
    "        optimizer           = 'nag',\n",
    "        optimizer_params   = optimizer_params,        \n",
    "        num_epoch           = numepoch, \n",
    "        initializer         = mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2),\n",
    "        )\n",
    "    \n",
    "    epoch_max_val_acc, train_acc, val_acc = max_val_epoch(output_filename)\n",
    "    #print \"val-acc: \" + str(val_acc) \n",
    "    \n",
    "    # Prediction on Training data\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix,epoch_max_val_acc)\n",
    "    model = mx.mod.Module(\n",
    "        context             = devs,\n",
    "        symbol              = sym,\n",
    "        data_names          = ['data'], \n",
    "        label_names         = workers_train_label.keys(),#['softmax0_label']\n",
    "        )\n",
    "    model.bind(for_training=False, data_shapes=train_iter.provide_data, \n",
    "         label_shapes=train_iter.provide_label,)\n",
    "    model.set_params(arg_params, aux_params, allow_missing=True)    \n",
    "\n",
    "    outputs = model.predict(train_iter)\n",
    "    if type(outputs) is list:\n",
    "        return outputs[0].asnumpy(), val_acc\n",
    "    else:\n",
    "        return outputs.asnumpy(), val_acc\n",
    "\n",
    "def max_val_epoch(filename):\n",
    "    import re\n",
    "    TR_RE = re.compile('.*?]\\sTrain-accuracy=([\\d\\.]+)')\n",
    "    VA_RE = re.compile('.*?]\\sValidation-accuracy=([\\d\\.]+)')\n",
    "    EPOCH_RE = re.compile('Epoch\\[(\\d+)\\] V+?')\n",
    "    log = open(filename, 'r').read()    \n",
    "    val_acc = [float(x) for x in VA_RE.findall(log)]\n",
    "    train_acc = [float(x) for x in TR_RE.findall(log)]\n",
    "    index_max_val_acc = np.argmax([float(x) for x in VA_RE.findall(log)])\n",
    "    epoch_max_val_acc = [int(x) for x in EPOCH_RE.findall(log)][index_max_val_acc]\n",
    "    return epoch_max_val_acc+1, train_acc[index_max_val_acc], val_acc[index_max_val_acc]\n",
    "\n",
    "class MultiIter(mx.io.DataIter):\n",
    "    def __init__(self, iter_list):\n",
    "        self.iters = iter_list \n",
    "        #self.batch_size = 500\n",
    "    def next(self):\n",
    "        batches = [i.next() for i in self.iters] \n",
    "        return mx.io.DataBatch(data=[t for t in batches[0].data],\n",
    "                         label= [t for t in batches[1].label],pad=0)\n",
    "    def reset(self):\n",
    "        for i in self.iters:\n",
    "            i.reset()\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return [t for t in self.iters[0].provide_data]\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return [t for t in self.iters[1].provide_label]\n",
    "    \n",
    "def multi_factor_scheduler(begin_epoch, epoch_size, step=[40, 50], factor=0.1):\n",
    "    step_ = [epoch_size * (x-begin_epoch) for x in step if x-begin_epoch > 0]\n",
    "    return mx.lr_scheduler.MultiFactorScheduler(step=step_, factor=factor) if len(step_) else None\n",
    "\n",
    "    \n",
    "\n",
    "'''\n",
    "Reproducing paper:\n",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. \"Identity Mappings in Deep Residual Networks\"\n",
    "'''\n",
    "def residual_unit(data, num_filter, stride, dim_match, name, bottle_neck=True, bn_mom=0.9, workspace=512, memonger=False):\n",
    "    \"\"\"Return ResNet Unit symbol for building ResNet\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        Input data\n",
    "    num_filter : int\n",
    "        Number of output channels\n",
    "    bnf : int\n",
    "        Bottle neck channels factor with regard to num_filter\n",
    "    stride : tupe\n",
    "        Stride used in convolution\n",
    "    dim_match : Boolen\n",
    "        True means channel number between input and output is the same, otherwise means differ\n",
    "    name : str\n",
    "        Base name of the operators\n",
    "    workspace : int\n",
    "        Workspace used in convolution operator\n",
    "    \"\"\"\n",
    "    if bottle_neck:\n",
    "        # the same as https://github.com/facebook/fb.resnet.torch#notes, a bit difference with origin paper\n",
    "        bn1 = mx.sym.BatchNorm(data=data, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_bn1')\n",
    "        act1 = mx.sym.Activation(data=bn1, act_type='relu', name=name + '_relu1')\n",
    "        conv1 = mx.sym.Convolution(data=act1, num_filter=int(num_filter*0.25), kernel=(1,1), stride=(1,1), pad=(0,0),\n",
    "                                      no_bias=True, workspace=workspace, name=name + '_conv1')\n",
    "        bn2 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_bn2')\n",
    "        act2 = mx.sym.Activation(data=bn2, act_type='relu', name=name + '_relu2')\n",
    "        conv2 = mx.sym.Convolution(data=act2, num_filter=int(num_filter*0.25), kernel=(3,3), stride=stride, pad=(1,1),\n",
    "                                      no_bias=True, workspace=workspace, name=name + '_conv2')\n",
    "        bn3 = mx.sym.BatchNorm(data=conv2, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_bn3')\n",
    "        act3 = mx.sym.Activation(data=bn3, act_type='relu', name=name + '_relu3')\n",
    "        conv3 = mx.sym.Convolution(data=act3, num_filter=num_filter, kernel=(1,1), stride=(1,1), pad=(0,0), no_bias=True,\n",
    "                                   workspace=workspace, name=name + '_conv3')\n",
    "        if dim_match:\n",
    "            shortcut = data\n",
    "        else:\n",
    "            shortcut = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(1,1), stride=stride, no_bias=True,\n",
    "                                            workspace=workspace, name=name+'_sc')\n",
    "        if memonger:\n",
    "            shortcut._set_attr(mirror_stage='True')\n",
    "        return conv3 + shortcut\n",
    "    else:\n",
    "        bn1 = mx.sym.BatchNorm(data=data, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn1')\n",
    "        act1 = mx.sym.Activation(data=bn1, act_type='relu', name=name + '_relu1')\n",
    "        conv1 = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(3,3), stride=stride, pad=(1,1),\n",
    "                                      no_bias=True, workspace=workspace, name=name + '_conv1')\n",
    "        bn2 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn2')\n",
    "        act2 = mx.sym.Activation(data=bn2, act_type='relu', name=name + '_relu2')\n",
    "        conv2 = mx.sym.Convolution(data=act2, num_filter=num_filter, kernel=(3,3), stride=(1,1), pad=(1,1),\n",
    "                                      no_bias=True, workspace=workspace, name=name + '_conv2')\n",
    "        if dim_match:\n",
    "            shortcut = data\n",
    "        else:\n",
    "            shortcut = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(1,1), stride=stride, no_bias=True,\n",
    "                                            workspace=workspace, name=name+'_sc')\n",
    "        if memonger:\n",
    "            shortcut._set_attr(mirror_stage='True')\n",
    "        return conv2 + shortcut\n",
    "\n",
    "def resnet(units, num_stage, filter_list, num_class, data_type, bottle_neck=True, bn_mom=0.9, workspace=512, memonger=False):\n",
    "    \"\"\"Return ResNet symbol of cifar10 and imagenet\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : list\n",
    "        Number of units in each stage\n",
    "    num_stage : int\n",
    "        Number of stage\n",
    "    filter_list : list\n",
    "        Channel size of each stage\n",
    "    num_class : int\n",
    "        Ouput size of symbol\n",
    "    dataset : str\n",
    "        Dataset type, only cifar10 and imagenet supports\n",
    "    workspace : int\n",
    "        Workspace used in convolution operator\n",
    "    \"\"\"\n",
    "    num_unit = len(units)\n",
    "    assert(num_unit == num_stage)\n",
    "    data = mx.sym.Variable(name='data')\n",
    "    data = mx.sym.BatchNorm(data=data, fix_gamma=True, eps=2e-5, momentum=bn_mom, name='bn_data')\n",
    "    if data_type == 'cifar10':\n",
    "        body = mx.sym.Convolution(data=data, num_filter=filter_list[0], kernel=(3, 3), stride=(1,1), pad=(1, 1),\n",
    "                                  no_bias=True, name=\"conv0\", workspace=workspace)\n",
    "    elif data_type == 'imagenet':\n",
    "        body = mx.sym.Convolution(data=data, num_filter=filter_list[0], kernel=(7, 7), stride=(2,2), pad=(3, 3),\n",
    "                                  no_bias=True, name=\"conv0\", workspace=workspace)\n",
    "        body = mx.sym.BatchNorm(data=body, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='bn0')\n",
    "        body = mx.sym.Activation(data=body, act_type='relu', name='relu0')\n",
    "        body = mx.symbol.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')\n",
    "    else:\n",
    "         raise ValueError(\"do not support {} yet\".format(data_type))\n",
    "    for i in range(num_stage):\n",
    "        body = residual_unit(body, filter_list[i+1], (1 if i==0 else 2, 1 if i==0 else 2), False,\n",
    "                             name='stage%d_unit%d' % (i + 1, 1), bottle_neck=bottle_neck, workspace=workspace,\n",
    "                             memonger=memonger)\n",
    "        for j in range(units[i]-1):\n",
    "            body = residual_unit(body, filter_list[i+1], (1,1), True, name='stage%d_unit%d' % (i + 1, j + 2),\n",
    "                                 bottle_neck=bottle_neck, workspace=workspace, memonger=memonger)\n",
    "    bn1 = mx.sym.BatchNorm(data=body, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='bn1')\n",
    "    relu1 = mx.sym.Activation(data=bn1, act_type='relu', name='relu1')\n",
    "    # Although kernel is not used here when global_pool=True, we should put one\n",
    "    pool1 = mx.symbol.Pooling(data=relu1, global_pool=True, kernel=(7, 7), pool_type='avg', name='pool1')\n",
    "    flat = mx.symbol.Flatten(data=pool1)\n",
    "    fc1 = mx.symbol.FullyConnected(data=flat, num_hidden=num_class, name='fc1')\n",
    "    softmax0 = mx.sym.log_softmax(fc1)\n",
    "    softmax0_output = mx.sym.BlockGrad(data = softmax0,name = 'softmax0')\n",
    "    loss = [softmax0_output]\n",
    "    label = mx.sym.Variable(name='softmax0_label')\n",
    "    ce = -mx.sym.sum(mx.sym.sum(mx.sym.broadcast_mul(softmax0,label),1))\n",
    "    loss[:] = loss +  [mx.symbol.MakeLoss(ce, normalization='batch')]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "fname = download_cifar10()\n",
    "# setting up values according to CIFAR10 dataset\n",
    "# n is total number of training samples for CIFAR10\n",
    "# n1 is the total number of test samples for CIFAR10 \n",
    "# k is the number of classes\n",
    "n, n1, k = 50000, 10000, 10\n",
    "    \n",
    "#setting the number of gpus that are available\n",
    "#gpus = '0,1,2,3' # if there are no gpus available set it to None.\n",
    "gpus=None\n",
    "# m is the number of workers,  gamma is the worker quality, \n",
    "# class_wise is the binary variable: takes value 1 if workers are class_wise hammer spammer \n",
    "# and 0 if workers are hammer-spammer\n",
    "# k is the number of classification classes, \n",
    "# epochs is the number of epochs for ResNet model\n",
    "m, gamma, class_wise, epochs, depth  = 100, 0.2, 0, 60, 20\n",
    "\n",
    "# calling  function to generate confusion matrices of workers\n",
    "conf = generate_workers(m,k,gamma,class_wise)  \n",
    "\n",
    "# calling the main function that takes as input the following:\n",
    "# name of .rec iterators and .lst files that to operate on,\n",
    "# worker confusion matrices, \n",
    "# number of epochs for running ResNet model, depth of the model,\n",
    "# number of gpus available on the machine,\n",
    "# samples: number of samples to be used for training the model,\n",
    "# repeat: the number of redundant noisy labels to be used for each training example, \n",
    "# that are generated using the worker confusion mtrices\n",
    "# it prints the generalization error of the model on set aside test data\n",
    "# note that the samples*repeat is approximately same for each pair\n",
    "# which implies that the total annotation budget is fixed.\n",
    "for repeat,samples in [[13,4000],[7,7000],[5,10000],[3,17000],[1,50000]]: \n",
    "    print \"\\nnumber of training examples: \" + str(samples) + \"\\t redundancy: \" + str(repeat)\n",
    "    # calling the main function\n",
    "    main(fname,n,n1,k,conf,samples,repeat,epochs,depth,gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
